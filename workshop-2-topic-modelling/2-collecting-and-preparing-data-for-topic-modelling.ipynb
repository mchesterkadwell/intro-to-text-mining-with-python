{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Preparing Text for Topic Modelling using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## More About Tokenising and Normalisation\n",
    "\n",
    "In the last workshop, in notebook `workshop-1-basics/2-collecting-and-preparing.ipynb`, we cleaned and prepared the text _The Iliad of Homer_ (translated by Alexander Pope (1899)) by:\n",
    "* Tokenising the text into individual words.\n",
    "* Normalising the text:\n",
    " * into lowercase,\n",
    " * removing punctuation,\n",
    " * removing non-words (empty strings, numerals, etc.),\n",
    " * removing stopwords.\n",
    "\n",
    "One form of normalisation we didn't do last time is making sure that different _inflections_ of the same word are counted together. In English, words are modified to express quantity, tense, etc. (i.e. _declension_ and _conjugation_ for those who remember their language lessons!).\n",
    "\n",
    "For example, 'fish', 'fishes', 'fishy' and 'fishing' are all formed from the root 'fish'. Last workshop, all these words would have been counted as different words, which may or may not be desirable.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "There are two main ways to normalise for inflection:\n",
    "\n",
    "* **Stemming** - reducing a word to a stem by removing endings (a **stem** may not be an actual word).\n",
    "* **Lemmatization** - reducing a word to its meaningful base form using its context (a **lemma** is typically a proper word in the language).\n",
    "\n",
    "To do this we can use several facilities provided by NLTK. There are many different ways to stem and lemmatize words, but we will compare the results of the [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) and [WordNet](https://wordnet.princeton.edu/) lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the H.G. Wells book _The First Men on the Moon_ from Project Gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE FIRST MEN IN THE MOON\\r\\n\\r\\nby H.G. Wells\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nChapter 1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMr. Bedford Meets Mr. Cavor at Lympne\\r\\n\\r\\nAs I sit down to write here amidst the shadows of vine-leaves under the\\r\\nblue sky of southern Italy, it com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/1/1013/1013.txt')\n",
    "text = response.text\n",
    "text[681:900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pick out one sentence from the book to use an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All about us on the sunlit slopes frothed and swayed the darting shrubs'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_wells = text[118017:118088]\n",
    "hg_wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenise the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slopes',\n",
       " 'frothed',\n",
       " 'and',\n",
       " 'swayed',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrubs']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(hg_wells)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the Porter Stemmer to find the word stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'about',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'froth',\n",
       " 'and',\n",
       " 'sway',\n",
       " 'the',\n",
       " 'dart',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(token) for token in tokens]\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare these stems with lemmas, we download the WordNet lemmatizer and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'frothed',\n",
       " 'and',\n",
       " 'swayed',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the results? Perhaps surprisingly, the lemmatizer seems to have performed more poorly than the stemmer since `frothed` and `darting` have not been reduced to `froth` and `dart`.\n",
    "\n",
    "The different rules used to stem and lemmatize words are called _algorithms_ and they can result in different stems and lemmas. If the precise details of this are important to your research, you should compare the results of the various algorithms. Stemmers and lemmatizers are also available in many languages, not just English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Going Further: Improving Lemmatization with Part-of-Speech (POS) Tagging\n",
    "\n",
    "To improve the lemmatizer's performance we can tell it which _part of speech_ each word is, which is known as **part-of-speech tagging (POS tagging)**. A part of speech is the role a word plays in the sentence, e.g. verb, noun, adjective, etc.\n",
    "\n",
    "NLTK has a POS tagger so let's download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('All', 'DT'),\n",
       " ('about', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sunlit', 'NN'),\n",
       " ('slopes', 'VBZ'),\n",
       " ('frothed', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('swayed', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('darting', 'NN'),\n",
       " ('shrubs', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the POS tags for each token\n",
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags that NLTK generates are from the [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/MBSP-tags). For example, now we know that `frothed` is a 'verb, past participle' (VBN).\n",
    "\n",
    "Unfortunately, the NLTK lemmatizer accepts WordNet tags (`ADJ, ADV, NOUN, VERB = 'a', 'r', 'n', 'v'`) instead! In theory, at least, if we pass the tagging information to the lemmatizer, the results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of tokens to WordNet POS tags\n",
    "tags = [('All', 'n'),\n",
    " ('about', 'n'),\n",
    " ('us', 'n'),\n",
    " ('on', 'n'),\n",
    " ('the', 'n'),\n",
    " ('sunlit', 'a'),\n",
    " ('slopes', 'v'),\n",
    " ('frothed', 'v'),\n",
    " ('and', 'n'),\n",
    " ('swayed', 'v'),\n",
    " ('the', 'n'),\n",
    " ('darting', 'a'),\n",
    " ('shrubs', 'n')]\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(*tag) for tag in tags]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `frothing` has been reduced to `froth`. In practice, however, we may wish to [experiment](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) with other lemmatizers to get the best results. The [SpaCy](https://spacy.io/) Python library has an excellent alternative lemmatizer, for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Going Further: Beyond NLTK to SpaCy\n",
    "\n",
    "NLTK was the first open-source Python library for Natural Language Processing (NLP), originally released in 2001, and it is still a valuable tool for teaching and research. Much of the literature uses NLTK code in its examples, which is why I chose to write this course using NLTK. As you may deduce from the parts-of-speech tagging example (above), NLTK does have its limitations though.\n",
    "\n",
    "In many ways NLTK has been overtaken in efficiency and ease of use by other, more modern libraries, such as [SpaCy](https://spacy.io/). SpaCy is designed to use less computer memory and split workloads across multiple processor cores (or even computers) so that it can handle very large corpora easily. It also has excellent documentation. If you are serious about text-mining with Python for a large research dataset, I recommend that you try SpaCy. If you have understood the text-mining principles we have covered with NLTK, you will have no trouble using SpaCy as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Gensim Python Library for Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/) is an open-source library that specialises in topic modelling. It is powerful, easy to use and is designed to work with very large corpora. (Another Python library, [scikit-learn](https://scikit-learn.org), also has topic modelling, but we won't cover that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Example Corpus: US Presidential Inaugural Addresses\n",
    "\n",
    "First, we are going to load a corpus of speeches `nltk.corpus.inaugural` that comes packaged into NLTK. This is the C-Span Inaugural Address Corpus (public domain) that contains the inaugural address of every US president from 1789â€“2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "inaugural = nltk.corpus.inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what is inside, we can list the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = inaugural.fileids()\n",
    "files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And examine the first few words of each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'am', 'again', ...]\n",
      "['When', 'it', 'was', 'first', 'perceived', ',', 'in', ...]\n",
      "['Friends', 'and', 'Fellow', 'Citizens', ':', 'Called', ...]\n",
      "['Proceeding', ',', 'fellow', 'citizens', ',', 'to', ...]\n",
      "['Unwilling', 'to', 'depart', 'from', 'examples', 'of', ...]\n",
      "['About', 'to', 'add', 'the', 'solemnity', 'of', 'an', ...]\n",
      "['I', 'should', 'be', 'destitute', 'of', 'feeling', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'shall', 'not', ...]\n",
      "['In', 'compliance', 'with', 'an', 'usage', 'coeval', ...]\n"
     ]
    }
   ],
   "source": [
    "for file in files[0:10]:\n",
    "    print(inaugural.words(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Corpora for Learning and Practicing Text-Mining\n",
    "It is difficult to source pre-prepared corpora for learning and practicing text-mining. The documents must be good quality, easily available and distributed with a license that allows text-mining. NLTK comes with a number of corpora you can download from [`nltk_data`](http://www.nltk.org/nltk_data/) but these are quite old and limited in scope. It's worth searching around for [lists of corpora](https://nlpforhackers.io/corpora/) but bear in mind you must determine the true source and licensing of any corpus for yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text in Gensim\n",
    "Before we can start to do topic modelling we must â€” of course! â€” clean and prepare the text by tokenising, removing stopwords, stemming, and so on. We could do this with NLTK, as we have learnt, but Gensim can do that for us too.\n",
    "\n",
    "The defaults of `preprocess_string()` and `preprocess_documents()` use the following _filters_:\n",
    "\n",
    "* Strip any HTML or XML tags\n",
    "* Replace punctuation characters with spaces\n",
    "* Remove repeating whitespace characters and turn tabs and line breaks into spaces\n",
    "* Remove digits\n",
    "* Remove stopwords\n",
    "* Remove words with length less than 3 characters\n",
    "* Lowercase\n",
    "* Stem the words using a Porter Stemmer\n",
    "\n",
    "Using Gensim, we will preprocess just the _first_ file in the corpus as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'senat',\n",
       " 'hous',\n",
       " 'repres',\n",
       " 'vicissitud',\n",
       " 'incid',\n",
       " 'life',\n",
       " 'event',\n",
       " 'fill']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "washington = files[0]\n",
    "text = inaugural.raw(washington)\n",
    "tokens = preprocess_string(text)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, what has happened here to our tokens? ðŸ˜•\n",
    "\n",
    "The Porter Stemmer that comes with Gensim does not give us real words, but this will make our topics less readable.\n",
    "\n",
    "We can do something about this, but the code is a bit more advanced. Feel free to skip over the next section and start reading again at 'Pre-Processing the Corpus and Saving to File'.\n",
    "\n",
    "---\n",
    "#### Going Further: Using SpaCy's Lemmatizer to Get Real Words\n",
    "\n",
    "In order to lemmatize the words instead, we have to specify a _list of filters_ that we want `preprocess_string()` to apply.\n",
    "\n",
    "Before that we will import an alternative lemmatizer from the [SpaCy](https://spacy.io/) library, as it is a better by default than the NLTK one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.1MB 5.6MB/s ta 0:00:011  19% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 2.1MB 1.3MB/s eta 0:00:07    92% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 10.3MB 5.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
      "/Users/mary/digitalhumanities/intro-to-text-mining-with-python/venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/mary/digitalhumanities/intro-to-text-mining-with-python/venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sway'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.lemmatizer import LOOKUP\n",
    "lemmatize = Lemmatizer(lookup=LOOKUP).lookup\n",
    "lemmatize('swayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ðŸ‘†ðŸ‘†ðŸ‘† All you need to understand here is that we using SpaCy's lemmatizer rather than NLTK's. If you don't understand the code, you can skip over it and continue.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a list of filters, which are in fact the same as defaults, except with the string method `lower()` and without the Gensim stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'senate',\n",
       " 'house',\n",
       " 'representative',\n",
       " 'among',\n",
       " 'vicissitude',\n",
       " 'incident',\n",
       " 'life',\n",
       " 'event']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = [strip_tags, \n",
    "           strip_punctuation, \n",
    "           strip_multiple_whitespaces, \n",
    "           strip_numeric, \n",
    "           remove_stopwords, \n",
    "           strip_short,\n",
    "           str.lower]\n",
    "\n",
    "# Pre-process the tokens with the filters\n",
    "tokens = preprocess_string(text, filters=filters)\n",
    "\n",
    "# Lemmatize the filtered tokens with SpaCy's lemmatizer\n",
    "lemmas = [lemmatize(token) for token in tokens]\n",
    "lemmas[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have real words for the tokens, instead of awkward stems. We'll use these lemmatized tokens for our topic modelling example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Pre-Processing the Corpus and Saving to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files and Writing to File\n",
    "Last workshop I glossed over how we save to text files read them back in again. I offered this guide [Reading and Writing Files in Python](https://realpython.com/read-write-files-python/#opening-and-closing-a-file-in-python), which is an excellent in-depth look that I recommend.\n",
    "\n",
    "In brief, in order to open files we use the `open()` function and the keyword `with`.\n",
    "\n",
    "For reading:\n",
    "\n",
    "`with open(file, 'r') as reader:`\n",
    "\n",
    "For writing:\n",
    "\n",
    "`with open(file, 'w') as writer:`\n",
    "\n",
    "Then whatever you put inside the code block will run with the file open and ready. Once your code has finished running the file is safely closed.\n",
    "\n",
    "We can create and then write a text file with the `write()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blackhole.txt', 'w') as writer:\n",
    "    writer.write('At the center of a black hole lies a singularity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now go to the Jupyter notebook folder `workshop-2-topic-modelling`, open the newly created text file `blackhole.txt` and inspect its contents!\n",
    "\n",
    "We can read this file back in to a string with the `read()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At the center of a black hole lies a singularity.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('blackhole.txt', 'r') as reader:\n",
    "    sentence = reader.read()\n",
    "    \n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write line by line (instead of the whole file at once) use `writelines()`, and likewise, to read one line at a time use `readlines()`. For all the details, see the tutorial linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Speeches and Saving Tokens to Text File\n",
    "We can now put everything we have learnt together to pre-process our entire corpus of speeches, and save the clean lemma tokens to text files, ready to be loaded in the next notebook `3-topic-modelling-and-visualising.ipynb`.\n",
    "\n",
    "Let's step through this code now:\n",
    "\n",
    "1. Create a location for the `data/inaugural` folder where we want to save the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "location = Path('data', 'inaugural-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loop over all the files in turn, using the Gensim `preprocess_string` function to prepare them, and save them as individual files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 1789-Washington.txt\n",
      "Processing file: 1793-Washington.txt\n",
      "Processing file: 1797-Adams.txt\n",
      "Processing file: 1801-Jefferson.txt\n",
      "Processing file: 1805-Jefferson.txt\n",
      "Processing file: 1809-Madison.txt\n",
      "Processing file: 1813-Madison.txt\n",
      "Processing file: 1817-Monroe.txt\n",
      "Processing file: 1821-Monroe.txt\n",
      "Processing file: 1825-Adams.txt\n",
      "Processing file: 1829-Jackson.txt\n",
      "Processing file: 1833-Jackson.txt\n",
      "Processing file: 1837-VanBuren.txt\n",
      "Processing file: 1841-Harrison.txt\n",
      "Processing file: 1845-Polk.txt\n",
      "Processing file: 1849-Taylor.txt\n",
      "Processing file: 1853-Pierce.txt\n",
      "Processing file: 1857-Buchanan.txt\n",
      "Processing file: 1861-Lincoln.txt\n",
      "Processing file: 1865-Lincoln.txt\n",
      "Processing file: 1869-Grant.txt\n",
      "Processing file: 1873-Grant.txt\n",
      "Processing file: 1877-Hayes.txt\n",
      "Processing file: 1881-Garfield.txt\n",
      "Processing file: 1885-Cleveland.txt\n",
      "Processing file: 1889-Harrison.txt\n",
      "Processing file: 1893-Cleveland.txt\n",
      "Processing file: 1897-McKinley.txt\n",
      "Processing file: 1901-McKinley.txt\n",
      "Processing file: 1905-Roosevelt.txt\n",
      "Processing file: 1909-Taft.txt\n",
      "Processing file: 1913-Wilson.txt\n",
      "Processing file: 1917-Wilson.txt\n",
      "Processing file: 1921-Harding.txt\n",
      "Processing file: 1925-Coolidge.txt\n",
      "Processing file: 1929-Hoover.txt\n",
      "Processing file: 1933-Roosevelt.txt\n",
      "Processing file: 1937-Roosevelt.txt\n",
      "Processing file: 1941-Roosevelt.txt\n",
      "Processing file: 1945-Roosevelt.txt\n",
      "Processing file: 1949-Truman.txt\n",
      "Processing file: 1953-Eisenhower.txt\n",
      "Processing file: 1957-Eisenhower.txt\n",
      "Processing file: 1961-Kennedy.txt\n",
      "Processing file: 1965-Johnson.txt\n",
      "Processing file: 1969-Nixon.txt\n",
      "Processing file: 1973-Nixon.txt\n",
      "Processing file: 1977-Carter.txt\n",
      "Processing file: 1981-Reagan.txt\n",
      "Processing file: 1985-Reagan.txt\n",
      "Processing file: 1989-Bush.txt\n",
      "Processing file: 1993-Clinton.txt\n",
      "Processing file: 1997-Clinton.txt\n",
      "Processing file: 2001-Bush.txt\n",
      "Processing file: 2005-Bush.txt\n",
      "Processing file: 2009-Obama.txt\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    \n",
    "    print(f'Processing file: {file}')\n",
    "    \n",
    "    text = inaugural.raw(file)\n",
    "    tokens = preprocess_string(text, filters=filters)\n",
    "    lemmas = [lemmatize(token) for token in tokens]\n",
    "    \n",
    "    with open(location / file, 'w') as writer:\n",
    "        writer.write(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feel free to inspect these files now in the folder `data/inaugural-test`. If for some reason you have changed the code and it's not worked properly, don't worry! I've created a proper set to use in `data/inaugural`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook we have covered:\n",
    "\n",
    "* Stemming and lemmatization\n",
    "* Gensim Python library for topic modelling\n",
    "* Pre-processing the text with Gensim\n",
    "* Reading from and writing to text files\n",
    "\n",
    "ðŸ‘ŒðŸ‘ŒðŸ‘Œ\n",
    "\n",
    "In the next notebook `3-topic-modelling-and-visualising.ipynb` we will walk through a full example of topic modelling using Gensim and the speeches we have prepared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
