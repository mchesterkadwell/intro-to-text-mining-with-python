{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Topic Modelling with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## What is Topic Modelling?\n",
    "\n",
    "Topic modelling is a _distant reading_ technique for finding structure in large collections of text, without actually reading everything by eye. If you have hundreds or thousands of documents and want to understand roughly what your corpus contains, then topic modelling may be for you.\n",
    "\n",
    "A topic modelling programme finds the words that appear frequently together in a document and groups them together to form 'topics'. A **topic** is a mixture of words that is supposed to characterise (part of) the content of a document â€” it's theme or underlying ideas. For example, one topic of this [Wikipedia article](https://en.wikipedia.org/wiki/Black_hole) is:\n",
    "\n",
    "* black, hole, mass, star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![First picture of a supermassive black hole, captured in 2019](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Black_hole_-_Messier_87.jpg/320px-Black_hole_-_Messier_87.jpg \"First picture of a supermassive black hole, captured in 2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too surprising, you may think. We could say the topic seems pretty accurate from our perspective. What about a document that we are less familiar with? Here is a topic of a [speech](https://er.jsc.nasa.gov/seh/ricetalk.htm) made by John F. Kennedy at Rice University in 1962:\n",
    "\n",
    "* space, new, year, man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Surveyor_3-Apollo_12.jpg/274px-Surveyor_3-Apollo_12.jpg \"Charles Conrad Jr., Apollo 12 Commander, examines the unmanned Surveyor III spacecraft on the Moon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Kennedy's famous 'we choose to go to the moon' speech. Notice that 'moon' is not in this topic; but the speech does cover the history of humankind's (\"man's\") endeavours and emphasises a forward-looking perspective (the \"new\"-ness of advancements).\n",
    "\n",
    "From these simplified examples, we can see that human intervention is still required to interpret what topics might 'mean'. Topic modelling is not magic; it is a tool that requires informed use and careful review, just like any other.\n",
    "\n",
    "### So... Why Do Topic Modelling?\n",
    "In the humanities, topic modelling may be used to support different approaches to large text corpora, such as:\n",
    "\n",
    "* Survey a collection that is too big to read closely e.g. [Computational Historiography: Data Mining in a Century of Classics Journals](http://www.perseus.tufts.edu/publications/02-jocch-mimno.pdf) (PDF)\n",
    "* Look at thematic trends over time in an archive e.g. [Topic Modeling Martha Ballard's Diary](http://www.cameronblevins.org/posts/topic-modeling-martha-ballards-diary/)\n",
    "* Create metadata for an archive to improve accessibility e.g. [Topic modelling for the valorisation of digitised archives of the European Commission](https://ieeexplore.ieee.org/abstract/document/7840981)\n",
    "* Understand current trends in social media relevant to your discipline e.g. [Mining the Open Web with â€˜Looted Heritageâ€™](https://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/)\n",
    "\n",
    "### Alternatives to Topic Modelling in Python\n",
    "If you are looking to explore the topics of a few documents in a casual way, you can use the online digital texts environment [Voyant](), which allows you to upload or copy-and-paste texts and explore a corpus with a number of graphical tools, including topics.\n",
    "\n",
    "For serious research, a well-known tool for topic modelling is called [MALLET](http://mallet.cs.umass.edu/topics.php), which is a programme (written in Java) that you download to your computer. You have to type commands to use MALLET, but it has otherwise done a great deal for you. [Getting Started with Topic Modeling and MALLET](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet) from Programming Historian gives a step-by-step tutorial on MALLET.\n",
    "\n",
    "There is a graphical interface for MALLET called [Topic Modeling Tool](https://github.com/senderle/topic-modeling-tool) that is a bit easier to use. The [Quickstart Guide](https://senderle.github.io/topic-modeling-tool/documentation/2017/01/06/quickstart.html) will get you up and running.\n",
    "\n",
    "If you are looking to use R rather than Python, then `tidytext` is a popular NLP library that will help you work with the `topicmodels` package. The book _Text Mining with R_ devotes [chapter 6](https://www.tidytextmining.com/topicmodeling.html) to this.\n",
    "\n",
    "---\n",
    "**With the alternatives out of the way, let's see how we can do topic modelling in Python!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## How to Join In with Coding\n",
    "\n",
    "* **Edit** any cell and try changing the code, or delete it and write your own.\n",
    "\n",
    "* Before running a cell, try to **guess** what the output will be by thinking through what will happen.\n",
    "\n",
    "* If you encounter an **error**, realise this is normal. Errors happen all the time and by reading the error message you will learn something new.\n",
    "\n",
    "* Remember: you cannot break the notebook or your computer, so **don't be afraid to experiment**.\n",
    "\n",
    "**Let's get coding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Recap of Python Basics\n",
    "Welcome back! Let's recap the Python that we learnt last time. Any questions?\n",
    "### Strings\n",
    "Create a _string_ and store it with a _name_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon formed 4.51 billion years ago.'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence = 'The Moon formed 4.51 billion years ago.'\n",
    "my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Slice_ a string. Remember that indexing in Python starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon formed 4.51'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a string with string methods. Important: the original string `my_sentence` is unchanged. Instead, a string method _returns_ a new string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tHE mOON FORMED 4.51 BILLION YEARS AGO.'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.swapcase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a string with string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence.islower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a string to see if it contains another string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'f' in my_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a _list_ of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Moon formed 4.51 billion years ago',\n",
       " \"The Moon is Earth's only permanent natural satellite\",\n",
       " 'The Moon was first reached in September 1959']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['The Moon formed 4.51 billion years ago',\n",
    "           \"The Moon is Earth's only permanent natural satellite\",\n",
    "          'The Moon was first reached in September 1959']\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Moon was first reached in September 1959'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformed list of strings with a _list comprehension_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"THE MOON IS EARTH'S ONLY PERMANENT NATURAL SATELLITE\"]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [string.upper() for string in my_list if 'Earth' in string]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "`import` a _module_ and use it. A module is simply code 'written by someone else' in another file (or files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE FIRST MEN IN THE MOON\\r\\n\\r\\nby H.G. Wells\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nChapter 1\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nMr. Bedford Meets Mr. Cavor at Lympne\\r\\n\\r\\nAs I sit down to write here amidst the shadows of vine-leaves under the\\r\\nblue sky of southern Italy, it com'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/1/1013/1013.txt')\n",
    "text = response.text\n",
    "text[681:900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import` [Natural Language Tool Kit](http://www.nltk.org/) (NLTK) to help with natural language processing (NLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'FIRST',\n",
       " 'MEN',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'MOON',\n",
       " 'by',\n",
       " 'H.G',\n",
       " '.',\n",
       " 'Wells',\n",
       " 'Chapter',\n",
       " '1',\n",
       " 'Mr.',\n",
       " 'Bedford',\n",
       " 'Meets',\n",
       " 'Mr.',\n",
       " 'Cavor',\n",
       " 'at',\n",
       " 'Lympne',\n",
       " 'As']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens[126:146]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "_Call_ a _function_ with _arguments_. For example, here the function `most_common()` takes a single argument `10`, to give us the ten most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4612),\n",
       " ('.', 3851),\n",
       " ('the', 3639),\n",
       " ('and', 2538),\n",
       " ('of', 2483),\n",
       " ('I', 2190),\n",
       " ('a', 1809),\n",
       " ('to', 1658),\n",
       " (\"''\", 1214),\n",
       " ('``', 1160)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freqdist = FreqDist(tokens)\n",
    "freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## More About Tokenising and Normalisation\n",
    "\n",
    "In the last workshop, in notebook `workshop-1-basics/2-collecting-and-preparing.ipynb`, we cleaned and prepared the text _The Iliad of Homer_ (translated by Alexander Pope (1899)) by:\n",
    "* Tokenising the text into individual words.\n",
    "* Normalising the text:\n",
    " * into lowercase,\n",
    " * removing punctuation,\n",
    " * removing non-words (empty strings, numerals, etc.),\n",
    " * removing stopwords.\n",
    "\n",
    "One form of normalisation we didn't do last time is making sure that different _inflections_ of the same word are counted together. In English, words are modified to express quantity, tense, etc. (i.e. _declension_ and _conjugation_ for those who remember their language lessons!).\n",
    "\n",
    "For example, 'fish', 'fishes', 'fishy' and 'fishing' are all formed from the root 'fish'. Last workshop, all these words would have been counted as different words, which may or may not be desirable.\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "There are two main ways to normalise for inflection:\n",
    "\n",
    "* **Stemming** - reducing a word to a stem by removing endings (a **stem** may not be an actual word).\n",
    "* **Lemmatization** - reducing a word to its meaningful base form using its context (a **lemma** is typically a proper word in the language).\n",
    "\n",
    "To do this we can use several facilities provided by NLTK. There are many different ways to stem and lemmatize words, but we will compare the results of the [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) and [WordNet](https://wordnet.princeton.edu/) lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All about us on the sunlit slopes frothed and swayed the darting shrubs'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_wells = text[118017:118088]\n",
    "hg_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'about',\n",
       " 'us',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'froth',\n",
       " 'and',\n",
       " 'sway',\n",
       " 'the',\n",
       " 'dart',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(hg_wells)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(token) for token in tokens]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mary/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slope',\n",
       " 'frothed',\n",
       " 'and',\n",
       " 'swayed',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the results? Perhaps surprisingly, the lemmatizer seems to have performed more poorly than the stemmer since `frothed` and `darting` have not been reduced to `froth` and `dart`.\n",
    "\n",
    "The different rules used to stem and lemmatize words are called _algorithms_ and they can result in different stems and lemmas. If the precise details of this are important to your research, you should compare the results of the various algorithms. Stemmers and lemmatizers are also available in many languages, not just English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Improving Lemmatization with Part-of-Speech Tagging\n",
    "\n",
    "To improve the lemmatizer's performance we can tell it which _part of speech_ each word is, which is known as **part-of-speech tagging (POS tagging)**. A part of speech is the role a word plays in the sentence, e.g. verb, noun, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the POS tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('All', 'DT'),\n",
       " ('about', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sunlit', 'NN'),\n",
       " ('slopes', 'VBZ'),\n",
       " ('frothed', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('swayed', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('darting', 'NN'),\n",
       " ('shrubs', 'NN')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the POS tags for each token\n",
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags that NLTK generates are from the [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/MBSP-tags). For example, now we know that `frothed` is a 'verb, past participle' (VBN).\n",
    "\n",
    "Unfortunately, the NLTK lemmatizer accepts WordNet tags (`ADJ, ADV, NOUN, VERB = 'a', 'r', 'n', 'v'`) instead! In theory, at least, if we pass the tagging information to the lemmatizer, the results are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All',\n",
       " 'about',\n",
       " 'u',\n",
       " 'on',\n",
       " 'the',\n",
       " 'sunlit',\n",
       " 'slop',\n",
       " 'froth',\n",
       " 'and',\n",
       " 'sway',\n",
       " 'the',\n",
       " 'darting',\n",
       " 'shrub']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping of tokens to WordNet POS tags\n",
    "tags = [('All', 'n'),\n",
    " ('about', 'n'),\n",
    " ('us', 'n'),\n",
    " ('on', 'n'),\n",
    " ('the', 'n'),\n",
    " ('sunlit', 'a'),\n",
    " ('slopes', 'v'),\n",
    " ('frothed', 'v'),\n",
    " ('and', 'n'),\n",
    " ('swayed', 'v'),\n",
    " ('the', 'n'),\n",
    " ('darting', 'a'),\n",
    " ('shrubs', 'n')]\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(*tag) for tag in tags]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `frothing` has been reduced to `froth`. In practice, however, we may wish to [experiment](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/) with other lemmatizers to get the best results. The [SpaCy](https://spacy.io/) Python library has an excellent alternative lemmatizer, for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Beyond NLTK to SpaCy\n",
    "\n",
    "NLTK was the first open-source Python library for Natural Language Processing (NLP), originally released in 2001, and it is still a valuable tool for teaching and research. Much of the literature uses NLTK code in its examples, which is why I chose to write this course using NLTK. As you may deduce from the parts-of-speech tagging example (above), NLTK does have its limitations though.\n",
    "\n",
    "In many ways NLTK has been overtaken in efficiency and ease of use by other, more modern libraries, such as [SpaCy](https://spacy.io/). SpaCy is designed to use less computer memory and split workloads across multiple processor cores (or even computers) so that it can handle very large corpora easily. It also has excellent documentation. If you are serious about text-mining with Python for a large research dataset, I recommend that you try SpaCy. If you have understood the text-mining principles we have covered with NLTK, you will have no trouble using SpaCy as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Gensim Python Library for Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gensim](https://radimrehurek.com/gensim/) is an open-source library that specialises in topic modelling. It is powerful, easy to use and is designed to work with very large corpora. (Another Python library, [scikit-learn](https://scikit-learn.org), also has topic modelling, but we won't cover that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Example Corpus: US Presidential Inaugural Addresses\n",
    "\n",
    "First, we are going to load a corpus of speeches `nltk.corpus.inaugural` that comes built in to NLTK. This is the C-Span Inaugural Address Corpus (public domain) that contains the inaugural address of every US president from 1789â€“2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to /home/mary/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('inaugural')\n",
    "inaugural = nltk.corpus.inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what is inside, we can list the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = inaugural.fileids()\n",
    "files[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And examine the first few words of each file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'am', 'again', ...]\n",
      "['When', 'it', 'was', 'first', 'perceived', ',', 'in', ...]\n",
      "['Friends', 'and', 'Fellow', 'Citizens', ':', 'Called', ...]\n",
      "['Proceeding', ',', 'fellow', 'citizens', ',', 'to', ...]\n",
      "['Unwilling', 'to', 'depart', 'from', 'examples', 'of', ...]\n",
      "['About', 'to', 'add', 'the', 'solemnity', 'of', 'an', ...]\n",
      "['I', 'should', 'be', 'destitute', 'of', 'feeling', ...]\n",
      "['Fellow', 'citizens', ',', 'I', 'shall', 'not', ...]\n",
      "['In', 'compliance', 'with', 'an', 'usage', 'coeval', ...]\n"
     ]
    }
   ],
   "source": [
    "for file in files[0:10]:\n",
    "    print(inaugural.words(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Going Further: Corpora for Learning and Practicing Text-Mining\n",
    "It is difficult to source pre-prepared corpora for learning and practicing text-mining. The documents must be good quality, easily available and distributed with a license that allows text-mining. NLTK comes with a number of corpora you can download from [`nltk_data`](http://www.nltk.org/nltk_data/) but these are quite old and limited in scope. It's worth searching around for [lists of corpora](https://nlpforhackers.io/corpora/) but bear in mind you must determine the true source and licensing of any corpus for yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text in Gensim\n",
    "Before we can start to do topic modelling we must â€” of course! â€” clean and prepare the text by tokenising, removing stopwords, stemming, and so on. We could do this with NLTK, as we have learnt, but Gensim can do that for us too.\n",
    "\n",
    "The defaults of `preprocess_string` and `preprocess_documents` uses the following _filters_:\n",
    "\n",
    "* Strip any HTML or XML tags\n",
    "* Replace punctuation characters with spaces\n",
    "* Remove repeating whitespace characters and turn tabs and line breaks into spaces\n",
    "* Remove digits\n",
    "* Remove stopwords\n",
    "* Remove words with length less than 3 characters\n",
    "* Lowercase\n",
    "* Stem the words using a Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'senat',\n",
       " 'hous',\n",
       " 'repres',\n",
       " 'vicissitud',\n",
       " 'incid',\n",
       " 'life',\n",
       " 'event',\n",
       " 'fill']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "# Pre-process the first file in the corpus as an example\n",
    "washington = files[0]\n",
    "text = inaugural.raw(washington)\n",
    "tokens = preprocess_string(text)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter Stemmer that comes with Gensim does not give us real words; this will make our topics less readable. In order to lemmatize the words instead, we have to specify a _list of filters_ that we want `preprocess_string` to apply.\n",
    "\n",
    "Before that we will import an alternative lemmatizer from the [SpaCy](https://spacy.io/) library, as it is a better by default than the NLTK one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sway'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.lemmatizer import LOOKUP\n",
    "lemmatize = Lemmatizer(lookup=LOOKUP).lookup\n",
    "lemmatize('swayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply a list of filters, which are in fact the same as defaults, except with the string method `lower()` and without the Gensim stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fellow',\n",
       " 'citizen',\n",
       " 'senate',\n",
       " 'house',\n",
       " 'representative',\n",
       " 'among',\n",
       " 'vicissitude',\n",
       " 'incident',\n",
       " 'life',\n",
       " 'event']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = [strip_tags, \n",
    "           strip_punctuation, \n",
    "           strip_multiple_whitespaces, \n",
    "           strip_numeric, \n",
    "           remove_stopwords, \n",
    "           strip_short,\n",
    "           str.lower]\n",
    "\n",
    "tokens = preprocess_string(text, filters=filters)\n",
    "lemmas = [lemmatize(token) for token in tokens]\n",
    "lemmas[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Python Essentials\n",
    "Before we go on to the next notebook `2-topic-modelling-fundamentals.ipynb` we need to a cover a few more Python essentials.\n",
    "\n",
    "### Looping with `for` loops\n",
    "We have already seen **`for` loops** in passing when we create new lists using list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ['rock', 'paper', 'scissors']\n",
    "new_list = [move for move in game]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is a special form of loop for comprehensions. The normal kind of `for` loop looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "paper\n",
      "scissors\n"
     ]
    }
   ],
   "source": [
    "for move in game:\n",
    "    print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `for` loop goes over every item in a list in turn â€” and runs whatever code is inside its _block_. It makes sure that _every_ item is visited, and then it _stops_ when it gets to the end. We call this **iteration**; the loop _iterates_ over the list. (Loops also work for many other types of things, like strings.)\n",
    "\n",
    "Note that a _block_ of code in Python is indicated by _indenting_ the code by several spaces (typically four spaces).\n",
    "\n",
    "### Saving to File\n",
    "Last workshop I glossed over how we save to text files. I offered this guide [Reading and Writing Files in Python](https://realpython.com/read-write-files-python/#opening-and-closing-a-file-in-python) as additional reading.\n",
    "\n",
    "Now we need to process our entire corpus and save the clean lemmas to text files\n",
    "\n",
    "I've Let's clean words into a file (`CLEAN-6130-8.txt`) and placed it in our project under the `data` folder. You can inspect it now if you like.\n",
    "\n",
    "The next involves saving some more files, so it is a good moment to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "text_file = Path('data', 'washington.txt')\n",
    "\n",
    "with open(text_file, 'w') as writer:\n",
    "    writer.write(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "Blah\n",
    "\n",
    "Blah: \n",
    "\n",
    "* sdfsdfsdf\n",
    "* sdfsdfsdf\n",
    "\n",
    "ðŸ‘ŒðŸ‘ŒðŸ‘Œ\n",
    "\n",
    "The next notebook ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
